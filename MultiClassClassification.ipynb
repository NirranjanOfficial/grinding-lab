{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN5iIiwv7ExVo10XK1iCNrB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0NhjIxrmEl_T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as functional\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "yBYuMF8L-nkv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle =True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle =True, num_workers=2)"
      ],
      "metadata": {
        "id": "WH6W3WU_-60_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89065b6e-4725-4a25-975f-ec7a2db1b1e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image , label = train_data[0]\n",
        "image.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMBjcrVF_9I_",
        "outputId": "df7f4c31-6d78-4a74-e071-b33d1adc1e6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['plane','cars','bird','cat','deer','dog','frog','horse','ship','truck']\n"
      ],
      "metadata": {
        "id": "RRg2Vw3WAQHX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_channels = 3,\n",
        "        out_channels = 12,\n",
        "        kernel_size = 5\n",
        "    )  #the 3 is taken from the image.size, we need 12x12 as output channel!\n",
        "    self.pool = nn.MaxPool2d(kernel_size = 2 , stride = 2) #(2,2)\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        in_channels = 12,\n",
        "        out_channels = 24,\n",
        "        kernel_size = 5\n",
        "        )            #nxt layer becomes = ([24,5,5])\n",
        "    self.fc1 = nn.Linear(in_features = 24*5*5, out_features = 120)  #flatterning it up!\n",
        "    self.fc2 = nn.Linear(in_features = 120, out_features = 84)\n",
        "    self.fc3 = nn.Linear(in_features = 84, out_features = 10) #10 is must ie 10 classes\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x =self.pool(functional.relu(self.conv1(x))) # we used ReLu to reduce linearity\n",
        "    x =self.pool(functional.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x,1)\n",
        "    x = functional.relu(self.fc1(x))\n",
        "    x = functional.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "84Aqf8FYAmnH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conv2d:\n",
        "After the conv layer we nned to know the current shape of the image\n",
        "  to do that we need to, ((Size-Kernel)//Stride+1)\n",
        "  Here the stride is the no of pikels our kernel moves one time!\n",
        "  ([3,32,32]) -> ([12,28,28])\n",
        "\n",
        "Maxpool2d:\n",
        "  after maxpool we get with current size/stride\n",
        "  ([12,28,28]) -> ([12,14,14])"
      ],
      "metadata": {
        "id": "--PxD_3BA_wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet()\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01,momentum=0.9)\n"
      ],
      "metadata": {
        "id": "dmQgqKV9Cpz1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(8):\n",
        "  print(f\"Training epoch:{epoch}...\")\n",
        "  running_loss = 0\n",
        "  for i, data in enumerate(train_loader):\n",
        "    inputs, labels = data\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(inputs)\n",
        "    loss = loss_function(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  print(f\"Loss:{running_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM4fO3NFDW8L",
        "outputId": "1e814a8d-09db-4e2a-c9d9-e97ac6dd3d9c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch:0...\n",
            "Loss:0.7426\n",
            "Training epoch:1...\n",
            "Loss:0.7039\n",
            "Training epoch:2...\n",
            "Loss:0.6726\n",
            "Training epoch:3...\n",
            "Loss:0.6455\n",
            "Training epoch:4...\n",
            "Loss:0.6173\n",
            "Training epoch:5...\n",
            "Loss:0.6095\n",
            "Training epoch:6...\n",
            "Loss:0.5906\n",
            "Training epoch:7...\n",
            "Loss:0.5725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now as the mmodel is trained we need to export and save it , but we dont need the entire model...so we need to save only the weight and bias so we can reuse it!"
      ],
      "metadata": {
        "id": "_ZTUxnfG6WQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net.state_dict(),'trained_net.pth')"
      ],
      "metadata": {
        "id": "_DJYpSQO58Hv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet()\n",
        "net.load_state_dict(torch.load('trained_net.pth'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6MmD86C6QjP",
        "outputId": "a90e78c2-0917-4af4-9ae6-1ae5c8f4ed5e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "net.eval()\n",
        "\n",
        "with torch.no_grad(): # to avoid Gradient calc since we arent training...\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    outputs = net(images) #our neural network applied to the images\n",
        "    _, predicted = torch.max(outputs.data, 1)  #onw thing with highest activation ie the label\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "#how many are classifed correctlyy!!\n",
        "\n",
        "accuracy = 100*correct/total\n",
        "print(f\"Accuracy:{accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve0vtAbP6qGx",
        "outputId": "e3c3ce85-d514-4f67-f2a3-12483673fb65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:66.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we might get lower accuracy as we gave low epoch...increase it !\n"
      ],
      "metadata": {
        "id": "uNobnQDm7fBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to unnormalize and show image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # Unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get a random index\n",
        "random_index = random.randint(0, len(test_data) - 1)\n",
        "\n",
        "# Get the image and label\n",
        "image, label = test_data[random_index]\n",
        "\n",
        "# Display the image\n",
        "print(\"Actual label:\", class_names[label])\n",
        "imshow(image)\n",
        "\n",
        "# Prepare image for model (add batch dimension)\n",
        "image_tensor = image.unsqueeze(0)  # Shape: [1, 3, 32, 32]\n",
        "\n",
        "# Set model to evaluation mode\n",
        "net.eval()\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    output = net(image_tensor)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    predicted_class = class_names[predicted.item()]\n",
        "\n",
        "print(\"Predicted label:\", predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "xUhRQ7LR7lsB",
        "outputId": "e404d4ba-c38c-4f26-c308-42c37f34ba86"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual label: cars\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: cars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This CNN isnt accurate enough...might need to increase the epoch to 5 or something...!"
      ],
      "metadata": {
        "id": "CXr5ScxY8VZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "need to look into this!\n",
        "Notes: Possible Improvements for the CIFAR-10 Classifier\n",
        "\n",
        "Use data augmentation (like random flips or crops) to improve generalization.\n",
        "\n",
        "Train for more epochs (e.g., 50–100) to allow the model to learn better.\n",
        "\n",
        "Replace SGD with Adam optimizer for faster and more stable convergence.\n",
        "\n",
        "Add regularization techniques such as Dropout and Batch Normalization to prevent overfitting.\n",
        "\n",
        "Use GPU for faster training if available.\n",
        "\n",
        "Visualize misclassified images to understand where the model is going wrong.\n",
        "\n",
        "Use pretrained models (transfer learning) like ResNet, VGG, or MobileNet for better performance.\n",
        "\n",
        "Track training and validation accuracy/loss using graphs for better debugging.\n",
        "\n",
        "Save and load the trained model for reuse without retraining.\n",
        "\n",
        "Evaluate the model using a confusion matrix and classification report for a deeper performance analysis."
      ],
      "metadata": {
        "id": "gKHGXMgw88Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ngbasSVM88pE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}